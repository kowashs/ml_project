\documentclass[paperwidth=40in,paperheight=32in,margin=1in,fontscale=0.33]{baposter}
\usepackage{amsmath}
\usepackage{amssymb}

% \usepackage{natbib}
\usepackage{enumitem}
\usepackage{subcaption}
\usepackage{float}
\usepackage{graphicx}
\usepackage[numbers]{natbib}
\usepackage{bibentry}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage[font=normalsize,labelfont=bf]{caption}
\usepackage{hyperref}

% \frenchspacing

\definecolor{box_color}{RGB}{126, 223, 231}
\definecolor{header_left}{RGB}{67, 148, 225}
\definecolor{header_right}{RGB}{77, 132, 199}
\definecolor{bg}{RGB}{215, 134, 57}


\renewcommand{\P}{\mathbb{P}}
\newcommand{\V}{\mathcal{V}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}


\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\DeclareMathOperator{\tr}{tr}

\nobibliography*



\setlist{nolistsep} % more compact spacing between environments
\setlist[itemize]{leftmargin=*} % nice margins for itemize ...

\setlength{\fboxsep}{1mm}

\begin{document}
\begin{poster}{
    headerColorOne=header_left, %
    headerColorTwo=header_right, %
    boxColorOne=box_color, %
    textborder=none, %
    headerborder=none, %
    background=plain, %
    bgColorOne=bg!10, %
    %% headers
    headerfont=\Large\sf\bf, %
    headerheight=0.11\textheight, %
    headershape=rounded, %
    %% other
    columns=3}{}{
	\Huge arXiv vs. snarXiv}{
	\Large Tyler Blanton and Sam Kowash\\University of Washington, Department of Physics}{\includegraphics[height=.75in]{figures/uw_block_purple.eps}}

\headerbox{Background}{name=background,column=0,row=0,span=2}{
	The arXiv is a popular e-print repository for publications in physics, astronomy, and other quantitative sciences. It hosts nearly 1.5 million papers, of which $\sim$120,000 are in theoretical high-energy physics (\texttt{hep-th}). Physicist David {Simmons-Duffin} developed a program known as the snarXiv, which procedurally generates abstracts in the style of \texttt{hep-th} from a context-free grammar.

	Humans (even physicists) have surprising difficulty determining whether a given abstract is from the arXiv or the snarXiv. Across 750,000 guesses collected through an online sorting game, players succeeded at picking the genuine paper from a pair only 59\% of the time. Try it for yourself below! \vspace{.25\baselineskip}


	\fcolorbox{black}{white}{
	\begin{minipage}[t]{.47\textwidth}
		{\small\bf Abstract A}

		\footnotesize
		In the 20th century, a fair amount of work was done demystifying QED in the presence of a stack of canonical co-isotropic branes. In this paper, we make contact with analyzing heterotic strings, consequently reconstructing perturbation theory on $\mathbb{C}^n$, and classify anomalous dimensions in loop models with sleptons. Our computation of the solution of magnetic dualities in models of hadrons provides a certain notion of perturbation theory (taking into account cosmic rays at $\Lambda_\mathrm{QCD}$). Our results prove that decay constants turn out to be equivalent to an instanton at the Planck scale. Finally, we establish that sleptons can be brought to bear in reformulating heavy ions.
		\vspace{.2\baselineskip}
	\end{minipage}}
	%
	\fcolorbox{black}{white}{
	\begin{minipage}[t]{.47\textwidth}
		{\small\bf Abstract B}

		\footnotesize
		We study the effective action of the heterotic string compactified on particular half-flat manifolds which arise in the context of mirror symmetry with NS-NS flux. We explicitly derive the superpotential and K{\"a}hler potential at lowest order in $\alpha'$ by a reduction of the bosonic action. The superpotential contains new terms depending on the K{\"a}hler moduli which originate from the intrinsic geometrical flux of the half-flat manifolds. A generalized Gukov formula, valid for all manifolds with SU(3) structure, is derived from the gravitino mass term. For the half-flat manifolds it leads to a superpotential in agreement with our explicit bosonic calculation. We also discuss the inclusion of gauge fields.
	\end{minipage}
	}

	\vspace{0.25\baselineskip}
	We explored computational approaches to this classification problem to investigate whether its difficulty stems from snarXiv's genius, or humans' bewilderment in the face of unfamiliar jargon. {\small(Explore snarXiv at: \url{https://github.com/davidsd/snarxiv}.)}
}



\headerbox{$n$-gram model}{name=ngram,column=0,row=0.38,span=1}{%
	One approach to text classification is to develop a \emph{language model}. Given a class $Y \in \{-1,1\}$ (with the negative sign referring to arXiv and the positive to snarXiv) and a document $X$ consisting of words $\{w_i\}_{i=1}^N$, we want to characterize the probability
	\vspace{-.5em}
	\begin{align*}
		\P(X | Y) &= \prod_{i=1}^N \P(w_i|w_1^{i-1}, Y),
	\end{align*}
	%
	where $w_1^{i-1}$ is the sequence of the first $i-1$ words. The sample space dwarfs the available data and this can never be satisfactorily trained. Instead we assume that a word's probability depends only on the preceding $n-1$ words, reducing the training task to estimating the probabilities of so-called $n$-grams from their frequencies in the training corpora.
	\vspace{-.5em}
	\begin{align*}
		\hat{\P}(X|Y) \equiv \prod_{i=1}^N \hat{\P}(w_i | w_{i-n+1}^{i-1}, Y) \equiv \frac{C_Y(w_{i-n+1}^i)+1}{C_Y(w_{i-n+1}^{i-1}) + V},
	\end{align*}
	%
	where $C_Y(w_{i-n+1}^i)$ is the number of times that $n$-gram occurs in the corpus for class $Y$ and $V$ is the size of our vocabulary. In our experiments we studied 1-grams (known as the bag-of-words model) and 2-grams.
	}


\headerbox{Naive Bayes}{name=nb,column=0,row=.84}{
	The simplest classifier based on $n$-gram frequencies sorts a document into whichever class has a higher posterior probability based on our estimate:
	\vspace{-.5em}
	\begin{align*}
		\hat{Y} &= \argmax_{Y\in\{-1,1\}} \P(Y) \hat{\P}(X|Y),
	\end{align*}
	%
	where $\P(Y)$ is the fraction of training abstracts in class $Y$.
}	

\headerbox{Likelihood-ratio test}{name=lr,column=1,row=.38}{
	The LR test improves upon naive Bayes by giving us a hyperparameter $\eta$ to adjust the sensitivity of the test. Given the ratio $\Lambda(X) \equiv \hat{P}(X|Y=1)/\hat{P}(X|Y=-1)$, the classifier $\delta^\ast_{LR}$ chooses class 1 (snarXiv) with probabilities
	%
	\begin{align*}
		\P(\delta^\ast_{LR}(X) = 1) &= \begin{cases}
									   	1, & \Lambda(X) > \eta\\
									   	1/2, &\Lambda(X) = \eta\\
									   	0, &\Lambda(X) < \eta\\
									   \end{cases}.
	\end{align*}
	%
	This reduces to the previous case when $\eta$ is the ratio of arXiv abstracts to snarXiv in the training set.
}


\headerbox{tf--idf}{name=tfidf, column=1, row=.64}{
	Another approach is to map documents into a continuous vector space and apply typical statistical learning methods. We used a tf--idf method, where each document gets a vector $d$ with an entry $d_i$ for each word in the vocabulary, weighted by
	%
	\begin{align*}
		d_i &= \mathrm{tf}_{i,d} \cdot \mathrm{idf}_i,
	\end{align*}
	%
	where we define
	%
	\begin{align*}
		  \mathrm{idf}_i &\equiv \log_{10} \frac{N+1}{\mathrm{df}_i + 1}, &
		  \mathrm{tf}_{i,d} &\equiv \begin{cases}
                         1 + \log_{10}C_{i,d} &:\quad C_{i,d} > 0\\
                         0 &:\quad\text{else}
                       \end{cases},
    \end{align*}
    %
    in which $C_{i,d}$ is the number of times $w_i$ appears in the document and $\mathrm{df}_i$ is the number of documents containing $w_i$. This weighting scheme is designed to focus on words which are less common in the training set, and ignore words like ``the'' and ``of'' which lack discriminating power. We used these vectors as features in an $L^2$-regularized logistic regression to classify abstracts.
}





\setlength{\fboxsep}{0mm}
\headerbox{Results}{name=results,column=2,row=0, span=1}{
{\small\textbf{\underline{Naive Bayes classifier}}: $\text{TCA} \approx 99.94\%$ ($\text{TPR} = 100\%$, $\text{FDR} \approx 32\%$)\\
\textbf{\underline{tf--idf + logistic regression}:} $\text{TCA} \approx 99.96\%$ for $\lambda=2.5\times10^{-8}$\\
\textbf{\underline{Likelihood-ratio test}:} $\text{TCA} \approx 99.98\%$ for $\eta_\text{PP}=1.4$}
	\vspace{-1em}
	\begin{figure}[H]
		\centering
		\fbox{\begin{subfigure}[t]{.49\textwidth}
			\centering
			\includegraphics[width=\textwidth]{figures/BoW_histogram.pdf}
		\end{subfigure}}
		%
		\fbox{\begin{subfigure}[t]{.49\textwidth}
			\centering
			\includegraphics[width=\textwidth]{figures/BoW_TPR_FDR.pdf}
		\end{subfigure}}

		\fbox{\begin{subfigure}[t]{.49\textwidth}
			\centering
			\includegraphics[width=\textwidth]{figures/bi_histogram.pdf}
		\end{subfigure}}
		%
		\fbox{\begin{subfigure}[t]{.49\textwidth}
			\centering
			\includegraphics[width=\textwidth]{figures/bi_TPR_FDR.pdf}
		\end{subfigure}}
	\end{figure}
%
For each model, we trained on 1,000 abstracts of each set and tested on 4,000 of each, using a vocabulary of 15,315 unique words that appeared at least twice in a separate training corpus of 12,000 abstracts from each set.
The naive Bayes classifier (using bag-of-words) is the worst performer, but still boasts a total classification accuracy (TCA) of 99.4\%, with the only misclassifications being false positives (arXiv mistaken for snarXiv).

The likelihood-ratio test (for both bag-of-words and bigrams) and logistic regression (on tf--idf) perform even better, each sporting a TCA greater than $99.95\%$.
Comparing the bag-of-words and bigram models in the likelihood-ratio test, we find that while both of the resulting classifiers are able to achieve a TCA of at least $99.98\%$ for appropriate choices of $\eta_\text{PP}$, the range of $\eta_\text{PP}$ which achieve these accuracies is significantly wider in the bigram model; in other words, adding context to words makes the contrast between arXiv and snarXiv abstracts much more pronounced.
}
\setlength{\fboxsep}{1mm}



\headerbox{Conclusions}{name=conclusions, column=2, row=.762}{
	Ultimately, we found that distinguishing arXiv from snarXiv is not just easier for computers, but \emph{trivial}. As we see in the histograms, the populations are statistically well-separated even in BoW analysis, but strikingly so when 2-grams introduce context. We expect this to sharpen with higher context, as the snarXiv produces long-range coherence only by random chance, and its juxtapositions of unrelated concepts will become more evident.
	}

\headerbox{References}{name=refs, column=2, row=.939}{
	\bibentry{jurafsky2014speech}
}



% \headerbox{References}{name=refs,column=2,row=.8}{
% 	\begingroup
% 	\renewcommand{\section}[2]{}
% 	\bibliographystyle{plainnat}
% 	\bibliography{lag_paper_new}
% 	\endgroup
% }
\nobibliography{references.bib}
\bibliographystyle{plainnat}
\end{poster}



\end{document}
\documentclass{article}
\usepackage[margin=1.0in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{braket,amsmath,amsfonts,mathtools,graphicx,subcaption,color,hyperref}
%\usepackage{scrextend}

\renewcommand{\P}{\mathbb{P}} % probability function

\title{Project Milestone: arXiv vs. snarXiv}
\author{Tyler Blanton, Samuel Kowash}
\date{November 15, 2018}


\begin{document}
\maketitle

\begin{abstract}
We give a brief update on our project of creating a program that accurately distinguishes between real arXiv \texttt{hep-th} abstracts and fake abstracts generated by the program snarXiv.
Our progress consists of three major parts: obtaining large amounts of data in an efficient way, parsing the abstract text to prepare it for analysis, and implementing a simple classification algorithm -- a naive Bayes classifier using a bag-of-words model.
We chose this extremely simple classifier to serve as a proof of concept, though it already outperforms humans (successful classification rate is 75-80\% vs. 57\% for humans).
\end{abstract}

\section{Obtaining abstracts}



\section{Parsing abstract text}
The raw response from an arXiv api call is a bunch of text consisting of full \texttt{hep-th} papers (which often contain TeX symbols like \$ and  \textbackslash) along with several tags specifying sections as title, author, abstract, etc.
For simplicity, we are currently only focusing on the abstracts, which we pull out and organize into a list using Python's \texttt{feedparser} package.
We also store snarXiv abstracts in this form, so the rest of the parsing process is the same for the arXiv and snarXiv data.

The next step is to split the abstract text into regular words that are easily identifiable between different abstracts.
We split on all spaces and newlines, and for uniformity we make all words lowercase and strip out all punctuation.
This method works for the most part, but it leaves traces of some TeX commands; for example, it sends \texttt{\textbackslash mathbb\{Z\} $\to$ mathbbz}.
In the future we may try a more sophisticated approach where we deal with TeX commands at the beginning before stripping all punctuation, but for now this issue is only a minor nuisance.
In any case, the output of our parsing procedure is two lists of lists (one arXiv, one snarXiv), where each inner list is comprised of formatted words from a single abstract.



\section{Classifying abstracts}
For our first attempt at creating a program that distinguishes between arXiv and snarXiv abstracts, we built a naive Bayes classifier.
The theory behind this classifier is simple:
assume that each abstract $X$ has a probability $\P(Y|X)$ of being from source $Y\in\{\text{arXiv, snarXiv}\}$.
If $\P(\text{arxiv}|X) > \P(\text{snarxiv}|X)$, then we should classify $X$ as arXiv; otherwise, we should classify it as snarXiv.
%\\
%\\
%\underline{Naive Bayes classifier}:
%\begin{addmargin}[1em]{2em}% 1em left, 2em right
%Assume that each abstract $X$ has a probability $\P(Y|X)$ of being from source $Y\in\{\text{arXiv, snarXiv}\}$.
%If $\P(\text{arxiv}|X) > \P(\text{snarxiv}|X)$, then classify $X$ as arXiv; otherwise, classify it as snarXiv.
%\end{addmargin}
This is an intuitive classification condition -- the only challenge is approximating $\P(Y|X)$ for any abstract $X$.
We can simplify matters by noting that
\begin{align}
	\P(Y|X) = \frac{\P(Y)\P(X|Y)}{\P(X)},
\end{align}
which means we should classify $X$ as arXiv iff
\begin{align}
	\P(\text{arxiv}|X) &> \P(\text{snarxiv}|X)  \\
	\Rightarrow	\P(\text{arxiv})\P(X|\text{arxiv}) &> \P(\text{snarxiv})\P(X|\text{snarxiv}). \label{Bayes}
\end{align}
We estimate each probability in \eqref{Bayes} by training on a large number of arXiv and snarXiv abstracts.
Since we are in control over how many arXiv vs. snarXiv abstracts will be in the train and test sets, $\P(Y)$ is not too interesting -- we ``approximate" it as the fraction of training abstracts which are from source $Y$, and its ``accuracy" is given by how close it is to the corresponding fraction of test abstracts which are from $Y$.

The less trivial task is estimating $\P(X|Y)$ -- the probability that source $Y$ will produce a given abstract $X$.
Perhaps the simplest estimation scheme is the bag-of-words model, in which each word $x_i\in X$ is assumed to be independent of all other words in the abstract.
This model is obviously flawed since certain word sequences are much more likely than others, but it can still be useful if some words are more likely in one source than another, so we adopt it to compute our initial estimate.
In this model, we define our training estimate $\widehat\P(X|Y)\approx\P(X|Y)$ as
\begin{align}
	\widehat\P(X|Y) &\equiv \prod_i \widehat\P(x_i|Y), \\
	\text{where } \quad \widehat\P(x_i|Y) &\equiv \frac{\#(x_i\in Y)_\text{train}}{\sum_j \#(x_j\in Y)_\text{train}}
\end{align} 
is the frequency with which word $x_i$ appears in source $Y$ inside the training set; $\#(x_i\in Y)$ is the number of times word $x_i$ appears in source $Y$ in the training set, and thus $\sum_j \#(x_j\in Y)$ is the total number of words in $Y$ inside the training set.

Testing this method on new abstracts after training on 800 arXiv and 800 snarXiv abstracts, we find that the algorithm correctly classifies an abstract as arXiv or snarXiv roughly 75-80\% of the time.
This is not great accuracy (which is to be expected given the simplicity of the model), but it is already higher than the average human classification accuracy of 57\%,%
\footnote{According to \url{http://snarxiv.org/vs-arxiv/highscores/}}
which is reassuring.


\end{document}